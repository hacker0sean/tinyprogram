2019-03-10 00:54:45.399931: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-03-10 00:54:45.632609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:03:00.0
totalMemory: 10.92GiB freeMemory: 10.48GiB
2019-03-10 00:54:45.632668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-03-10 00:54:45.902200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-10 00:54:45.902269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-03-10 00:54:45.902284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-03-10 00:54:45.903400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10134 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
start 2019-03-10 00:54:15
accusation
/tmp/pycharm_project_414/Model/model.conf
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Embedding (Embedding)        (None, 400, 512)          40960512  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 400, 512)          786944    
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 512)               0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
dense_1 (Dense)              (None, 1000)              513000    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 183)               183183    
=================================================================
Total params: 42,445,687
Trainable params: 42,444,663
Non-trainable params: 1,024
_________________________________________________________________
Epoch 1/1
 - 599s - loss: 0.3513
0 Accu:
0.9171410869387325
0.9066317524519832
0.8625135896566639
0 f1:
0.7648807966961242
0.7695347096982992
0.762863897521934
   0         1         2         3
0  0  0.917141  0.906632  0.862514
   0         1         2         3
0  0  0.764881  0.769535  0.762864
Epoch 1/1
 - 602s - loss: 0.2632
1 Accu:
0.9190699414329635
0.9109219924482425
0.8793004687700923
1 f1:
0.7811232596395601
0.7804422201822714
0.7855786972146255
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
Epoch 1/1
 - 601s - loss: 0.2237
2 Accu:
0.9194089522228587
0.9119390248179279
0.8838946494745332
2 f1:
0.789434255535747
0.7942736880426742
0.7791579691340724
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
Epoch 1/1
 - 600s - loss: 0.1961
3 Accu:
0.9187309306430684
0.9096477794793262
0.9034637550705493
3 f1:
0.7870456577834561
0.788043259565957
0.7882983080335203
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
3  3  0.918731  0.909648  0.903464
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
3  3  0.787046  0.788043  0.788298
Epoch 1/1
 - 600s - loss: 0.1780
4 Accu:
0.9197947231217048
0.9036157254246402
0.9091918684170535
4 f1:
0.8039080425774607
0.7998655287721103
0.809586475287382
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
3  3  0.918731  0.909648  0.903464
4  4  0.919795  0.903616  0.909192
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
3  3  0.787046  0.788043  0.788298
4  4  0.803908  0.799866  0.809586
Epoch 1/1
 - 598s - loss: 0.1661
5 Accu:
0.9197947231217048
0.9023532024829618
0.9100803104871235
5 f1:
0.8008570580242311
0.8032065570915308
0.8013211367968835
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
3  3  0.918731  0.909648  0.903464
4  4  0.919795  0.903616  0.909192
5  5  0.919795  0.902353  0.910080
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
3  3  0.787046  0.788043  0.788298
4  4  0.803908  0.799866  0.809586
5  5  0.800857  0.803207  0.801321
Epoch 1/1
 - 599s - loss: 0.1581
6 Accu:
0.9209403457910057
0.9017336310393603
0.9153174426896414
6 f1:
0.800165999654378
0.7911156383176108
0.805038449181497
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
3  3  0.918731  0.909648  0.903464
4  4  0.919795  0.903616  0.909192
5  5  0.919795  0.902353  0.910080
6  6  0.920940  0.901734  0.915317
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
3  3  0.787046  0.788043  0.788298
4  4  0.803908  0.799866  0.809586
5  5  0.800857  0.803207  0.801321
6  6  0.800166  0.791116  0.805038
Epoch 1/1
 - 601s - loss: 0.1519
7 Accu:
0.9209169657365301
0.9036624855335913
0.9199466934757958
7 f1:
0.8046551139617013
0.7880452691696407
0.8202547790641774
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
3  3  0.918731  0.909648  0.903464
4  4  0.919795  0.903616  0.909192
5  5  0.919795  0.902353  0.910080
6  6  0.920940  0.901734  0.915317
7  7  0.920917  0.903662  0.919947
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
3  3  0.787046  0.788043  0.788298
4  4  0.803908  0.799866  0.809586
5  5  0.800857  0.803207  0.801321
6  6  0.800166  0.791116  0.805038
7  7  0.804655  0.788045  0.820255
Epoch 1/1
 - 600s - loss: 0.1474
8 Accu:
0.9216768175069848
0.8945091942064225
0.9222145587599219
8 f1:
0.803211168556593
0.7898974601974238
0.8281987612106645
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
3  3  0.918731  0.909648  0.903464
4  4  0.919795  0.903616  0.909192
5  5  0.919795  0.902353  0.910080
6  6  0.920940  0.901734  0.915317
7  7  0.920917  0.903662  0.919947
8  8  0.921677  0.894509  0.922215
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
3  3  0.787046  0.788043  0.788298
4  4  0.803908  0.799866  0.809586
5  5  0.800857  0.803207  0.801321
6  6  0.800166  0.791116  0.805038
7  7  0.804655  0.788045  0.820255
8  8  0.803211  0.789897  0.828199
Epoch 1/1
 - 600s - loss: 0.1438
9 Accu:
0.921220906444712
0.9002723776346399
0.9197012029038028
9 f1:
0.8025778217158976
0.7913886069014259
0.8200287436515323
   0         1         2         3
0  0  0.917141  0.906632  0.862514
1  1  0.919070  0.910922  0.879300
2  2  0.919409  0.911939  0.883895
3  3  0.918731  0.909648  0.903464
4  4  0.919795  0.903616  0.909192
5  5  0.919795  0.902353  0.910080
6  6  0.920940  0.901734  0.915317
7  7  0.920917  0.903662  0.919947
8  8  0.921677  0.894509  0.922215
9  9  0.921221  0.900272  0.919701
   0         1         2         3
0  0  0.764881  0.769535  0.762864
1  1  0.781123  0.780442  0.785579
2  2  0.789434  0.794274  0.779158
3  3  0.787046  0.788043  0.788298
4  4  0.803908  0.799866  0.809586
5  5  0.800857  0.803207  0.801321
6  6  0.800166  0.791116  0.805038
7  7  0.804655  0.788045  0.820255
8  8  0.803211  0.789897  0.828199
9  9  0.802578  0.791389  0.820029
Epoch 1/1
 - 601s - loss: 0.1411
10 Accu:
0.9245058040985236
0.8960522778018073
0.9295792759197129
10 f1:
0.8115381003249033
0.7806978376346005
0.8199441021096114
     0         1         2         3
0    0  0.917141  0.906632  0.862514
1    1  0.919070  0.910922  0.879300
2    2  0.919409  0.911939  0.883895
3    3  0.918731  0.909648  0.903464
4    4  0.919795  0.903616  0.909192
5    5  0.919795  0.902353  0.910080
6    6  0.920940  0.901734  0.915317
7    7  0.920917  0.903662  0.919947
8    8  0.921677  0.894509  0.922215
9    9  0.921221  0.900272  0.919701
10  10  0.924506  0.896052  0.929579
     0         1         2         3
0    0  0.764881  0.769535  0.762864
1    1  0.781123  0.780442  0.785579
2    2  0.789434  0.794274  0.779158
3    3  0.787046  0.788043  0.788298
4    4  0.803908  0.799866  0.809586
5    5  0.800857  0.803207  0.801321
6    6  0.800166  0.791116  0.805038
7    7  0.804655  0.788045  0.820255
8    8  0.803211  0.789897  0.828199
9    9  0.802578  0.791389  0.820029
10  10  0.811538  0.780698  0.819944
Epoch 1/1
 - 600s - loss: 0.1389
11 Accu:
0.9225769496042926
0.8947897548601288
0.9195726126041873
11 f1:
0.8040904805647551
0.789759936170088
0.8201530519696245
     0         1         2         3
0    0  0.917141  0.906632  0.862514
1    1  0.919070  0.910922  0.879300
2    2  0.919409  0.911939  0.883895
3    3  0.918731  0.909648  0.903464
4    4  0.919795  0.903616  0.909192
5    5  0.919795  0.902353  0.910080
6    6  0.920940  0.901734  0.915317
7    7  0.920917  0.903662  0.919947
8    8  0.921677  0.894509  0.922215
9    9  0.921221  0.900272  0.919701
10  10  0.924506  0.896052  0.929579
11  11  0.922577  0.894790  0.919573
     0         1         2         3
0    0  0.764881  0.769535  0.762864
1    1  0.781123  0.780442  0.785579
2    2  0.789434  0.794274  0.779158
3    3  0.787046  0.788043  0.788298
4    4  0.803908  0.799866  0.809586
5    5  0.800857  0.803207  0.801321
6    6  0.800166  0.791116  0.805038
7    7  0.804655  0.788045  0.820255
8    8  0.803211  0.789897  0.828199
9    9  0.802578  0.791389  0.820029
10  10  0.811538  0.780698  0.819944
11  11  0.804090  0.789760  0.820153
Epoch 1/1
 - 599s - loss: 0.1368
Using TensorFlow backend.
/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
12 Accu:
0.9225184994681037
0.89384286265387
0.9239329927638731
12 f1:
0.801570905493683
0.7852825843093331
0.8014115764348668
     0         1         2         3
0    0  0.917141  0.906632  0.862514
1    1  0.919070  0.910922  0.879300
2    2  0.919409  0.911939  0.883895
3    3  0.918731  0.909648  0.903464
4    4  0.919795  0.903616  0.909192
5    5  0.919795  0.902353  0.910080
6    6  0.920940  0.901734  0.915317
7    7  0.920917  0.903662  0.919947
8    8  0.921677  0.894509  0.922215
9    9  0.921221  0.900272  0.919701
10  10  0.924506  0.896052  0.929579
11  11  0.922577  0.894790  0.919573
12  12  0.922518  0.893843  0.923933
     0         1         2         3
0    0  0.764881  0.769535  0.762864
1    1  0.781123  0.780442  0.785579
2    2  0.789434  0.794274  0.779158
3    3  0.787046  0.788043  0.788298
4    4  0.803908  0.799866  0.809586
5    5  0.800857  0.803207  0.801321
6    6  0.800166  0.791116  0.805038
7    7  0.804655  0.788045  0.820255
8    8  0.803211  0.789897  0.828199
9    9  0.802578  0.791389  0.820029
10  10  0.811538  0.780698  0.819944
11  11  0.804090  0.789760  0.820153
12  12  0.801571  0.785283  0.801412
Epoch 1/1
Traceback (most recent call last):
  File "model_tiaoli.py", line 168, in <module>
    history = model.fit(x=fact_train, y=labels_train, batch_size=batch_size, epochs=1, verbose=2)
  File "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py", line 1039, in fit
    validation_steps=validation_steps)
  File "/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py", line 199, in fit_loop
    outs = f(ins_batch)
  File "/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py", line 2715, in __call__
    return self._call(inputs)
  File "/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1439, in __call__
    run_metadata_ptr)
KeyboardInterrupt
